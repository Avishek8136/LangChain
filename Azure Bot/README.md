
# LangChain Azure OpenAI Integration Demo 🤖✨

This project demonstrates the integration of LangChain with Azure OpenAI's GPT-4 model, providing an interactive web interface using Streamlit. The application enables users to input queries on various topics, and it responds with answers generated by the GPT-4 model. It also introduces you to the LangChain framework for building multi-agent systems with language models.

## Problem Definition 🧠

The purpose of this project is to showcase how LangChain can be used to interact with OpenAI's GPT-4 model deployed on Azure to create a simple question-answering system. This integration serves as a hands-on introduction to LangChain, helping users learn how to build applications with language models using customized prompts, parsing, and model configurations.

The project utilizes Azure OpenAI's API to handle the requests and responses from the GPT-4 model, and integrates it with Streamlit for creating an easy-to-use web interface. The goal is to allow users to input questions, and get responses from the GPT-4 model based on the query.

## Key Features 🌟
- **Streamlit Interface**: Provides an interactive UI for querying the model.
- **Azure OpenAI API Integration**: Utilizes the Azure OpenAI service for GPT-4 model access.
- **LangChain Framework**: Uses LangChain to manage the prompt generation, LLM invocation, and output parsing.

## Technologies Used 💻
- **LangChain**: A framework designed to simplify the creation of applications with language models. LangChain makes it easy to create complex chains of LLMs and integrate them into apps.
- **Streamlit**: A Python framework to build interactive, data-driven applications in a few lines of code.
- **Azure OpenAI**: Provides access to OpenAI's powerful GPT-4 model via the Azure platform.
- **dotenv**: For loading environment variables, such as API keys and endpoint details, securely.
- **Python 3.x**: Programming language used for developing the project.

## Project Structure 📂

```
/LangChain_Azure_OpenAI_Demo
│
├── app.py                # Main Streamlit app code
├── .env                  # Environment file to store API keys
├── requirements.txt      # Python dependencies
└── README.md             # Project documentation
```

## Setup and Installation 🚀

1. **Clone the repository**:

   ```
   git clone https://github.com/your-repo/LangChain_Azure_OpenAI_Demo.git
   cd LangChain_Azure_OpenAI_Demo
   ```

2. **Install required dependencies**:

   It is recommended to create a virtual environment before installing dependencies.

   ```
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:

   Create a `.env` file in the root directory and add your Azure OpenAI API credentials:

   ```
   AZURE_OPENAI_API_KEY=your_azure_openai_api_key
   ```

4. **Run the Streamlit app**:

   Start the Streamlit app using the following command:

   ```
   streamlit run app.py
   ```

   This will launch the app in your default web browser.

## How to Use 🧑‍💻

1. Open the app in your web browser (it will automatically open on `http://localhost:8501`).
2. Type a query in the input box and hit enter.
3. The system will respond with an answer generated by the GPT-4 model from Azure OpenAI.
4. You can continue asking different questions, and the assistant will provide helpful answers based on the input.

Alternatively, you can access the deployed app directly here: [LangChain Azure OpenAI Demo](https://langchainazureopenai.streamlit.app/).

## LangChain Framework Overview 🔗

- **Prompt Template**: The `prompt_template` defines how the assistant should respond to queries. It uses placeholders like `{question}` to format the input dynamically.
- **Chain Construction**: LangChain allows you to create chains of actions, such as prompting the model, invoking the model, and parsing the output. In this case, the `prompt | llm | output_parser` creates a simple chain that generates a response using the GPT-4 model and formats the result.
- **Azure OpenAI LLM**: The `AzureChatOpenAI` class is used to call the GPT-4 model, passing the query and receiving the response.

## Learning Outcomes 📚

This project serves as an excellent learning resource for:
- Understanding how to interact with OpenAI models deployed on Azure using LangChain.
- Building web-based applications that interface with AI models.
- Learning about LangChain's structure for chaining model interactions, input/output handling, and prompt generation.
- Familiarizing with environment setup and API key management for cloud-based services.

## Future Improvements 🚀

- Add user authentication for personalized experiences.
- Expand the model’s capabilities with more advanced prompt engineering or additional models.
- Optimize response handling with more advanced output parsers.
- Deploy the app to a cloud platform for production use.

## References 📖

- [LangChain Documentation](https://www.langchain.com/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Azure OpenAI API Documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/)

